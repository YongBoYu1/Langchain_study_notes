{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Summary\n",
    "This Notebook demo the usecase from quick start summary from the LangChain.\\\n",
    "The official URL:\n",
    "https://python.langchain.com/docs/get_started/quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "import langchain_util as util\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain import hub\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load environemnt variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .env file which contains the API keys\n",
    "load_dotenv()\n",
    "\n",
    "# Set the API keys as environment variables\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ['LANGCHAIN_TRACING_V2']= 'true'\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv('TAVILY_API_KEY')\n",
    "\n",
    "\n",
    "# Optional, add tracing in LangSmith.\n",
    "# This will help you visualize and debug the control flow\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_PROJECT\"] = \"Agentic_RAG_LANGGRAPH\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can uncomment the following lines to test the tools, but this is damn expensive.\n",
    "# 2 runs costs aboout 4/1000 free token, where to upgrade you need to pay $100 for 15,000 tokens.\n",
    "# for the cheapest option. So, be careful with the usage.\n",
    "# Good luck!\n",
    "\n",
    "# from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# tool = TavilySearchResults()\n",
    "\n",
    "# tool.invoke({\"query\": \"What is tehr wheather in Toronto today?\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baisc chain usage\n",
    "No to useful, might just google for single question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# llm.invoke(\"What is ChatPromptTemplate?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain with prompt. The output is formatted with StrOutputParser\n",
    "Similar to the above. But you can prompt the llm to format your outcome a bit more. Looks a bit nicer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Langsmith can help with testing in several ways:\\n\\n1. Automated Testing: Langsmith can generate test scripts to automate the testing process, making it faster and more efficient. These scripts can be tailored to specific requirements and scenarios, helping to identify bugs and issues in the software.\\n\\n2. Test Data Generation: Langsmith can generate test data that covers a wide range of input parameters and edge cases. This helps to ensure comprehensive test coverage and thorough validation of the software.\\n\\n3. Performance Testing: Langsmith can help with performance testing by generating load and stress test scenarios. This can help to identify potential bottlenecks and performance issues in the software.\\n\\n4. Regression Testing: Langsmith can generate scripts for regression testing to ensure that new code changes do not introduce new bugs or issues into the software.\\n\\n5. Continuous Integration: Langsmith can integrate with CI/CD pipelines to automate the testing process and provide quick feedback on code changes. This helps to ensure that software quality is maintained throughout the development lifecycle.\\n\\nOverall, Langsmith can help streamline testing processes, improve test coverage, and ensure the overall quality of the software being developed.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", \"You are world class technical documentation writer.\"),\n",
    "#     (\"user\", \"{input}\")\n",
    "# ])\n",
    "\n",
    "# output_parser = StrOutputParser()\n",
    "# chain = prompt | llm | output_parser\n",
    "\n",
    "# chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Retrive Message \n",
    "The RAG staart to get a little useful from here. This method uses the llm to generate answer base on the document you provided. I am using a sigle url, but one of the useful way to using this is to have relevant documents into a dir. Using the llm to handle entire dir might be a more practical usecase. Now the document can be in pdf, csv, md... the [llamaindex](https://www.llamaindex.ai/) allow you passing different format of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # The llm\n",
    "# llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# # Build the vector database and set as retriever\n",
    "# vector_db = util.build_db()\n",
    "# retriever = vector_db.as_retriever()\n",
    "\n",
    "# # set up the chain that takes a question and the retrieved documents and \n",
    "# # generates an answer.\n",
    "# prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "# <context>\n",
    "# {context}\n",
    "# </context>\n",
    "\n",
    "# Question: {input}\"\"\")\n",
    "\n",
    "# # Takse a llm and temnplate.\n",
    "# # There are different type of chains available in the langchain.\n",
    "# # check out https://python.langchain.com/docs/modules/chains for detail\n",
    "# document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "# response = retrieval_chain.invoke({\"input\": \"What are different LCEL chains??\"})\n",
    "                                \n",
    "# print(response['answer'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation Retrieval Chain\n",
    "Passing conversation and prompt into the chain so we can have conversation with the llm, like the one online.\\\n",
    "We will need the pass the chat history into the chain. \\\n",
    "Perhaps that we need update the question and answer into history after retreie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='Is LCEL a great tool?'),\n",
       "  AIMessage(content='Yes!')],\n",
       " 'input': 'Tell me how',\n",
       " 'context': [Document(page_content='Skip to main contentDocsUse casesIntegrationsGuidesAPIMorePeopleVersioningChangelogContributingTemplatesCookbooksTutorialsYouTubeðŸ¦œï¸ðŸ”—LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsðŸ’¬SearchGet startedIntroductionInstallationQuickstartSecurityLangChain Expression LanguageGet startedWhy use LCELInterfaceStreamingHow toCookbookLangChain Expression Language (LCEL)ModulesModel I/ORetrievalAgentsChainsMoreLangServeLangSmithLangGraphModulesChainsChainsChains refer to sequences of calls - whether to an LLM, a tool, or a\\ndata preprocessing step. The primary supported way to do this is with\\nLCEL.LCEL is great for constructing your own chains, but itâ€™s also nice to\\nhave chains that you can use off-the-shelf. There are two types of\\noff-the-shelf chains that LangChain supports:Chains that are built with LCEL. In this case, LangChain offers a\\nhigher-level constructor method. However, all that is being done\\nunder the hood is constructing a chain with LCEL.[Legacy] Chains constructed by subclassing from a legacy Chain\\nclass. These chains do not use LCEL under the hood but are rather\\nstandalone classes.We are working creating methods that create LCEL versions of all chains.\\nWe are doing this for a few reasons.Chains constructed in this way are nice because if you want to\\nmodify the internals of a chain you can simply modify the LCEL.These chains natively support streaming, async, and batch out of the\\nbox.These chains automatically get observability at each step.This page contains two lists. First, a list of all LCEL chain\\nconstructors. Second, a list of all legacy Chains.LCEL Chains\\u200bBelow is a table of all LCEL chain constructors. In addition, we report\\non:Chain ConstructorThe constructor function for this chain. These are all methods that\\nreturn LCEL runnables. We also link to the API documentation.Function CallingWhether this requires OpenAI function calling.Other ToolsWhat other tools (if any) are used in this chain.When to UseOur commentary on when to use this chain.Chain ConstructorFunction CallingOther ToolsWhen to Usecreate_stuff_documents_chainThis chain takes a list of documents and formats them all into a prompt, then passes that prompt to an LLM. It passes ALL documents, so you should make sure it fits within the context window the LLM you are using.create_openai_fn_runnableâœ…If you want to use OpenAI function calling to OPTIONALLY structured an output response. You may pass in multiple functions for it call, but it does not have to call it.create_structured_output_runnableâœ…If you want to use OpenAI function calling to FORCE the LLM to respond with a certain function. You may only pass in one function, and the chain will ALWAYS return this response.load_query_constructor_runnableCan be used to generate queries. You must specify a list of allowed operations, and then will return a runnable that converts a natural language query into those allowed operations.create_sql_query_chainSQL DatabaseIf you want to construct a query for a SQL database from natural language.create_history_aware_retrieverRetrieverThis chain takes in conversation history and then uses that to generate a search query which is passed to the underlying retriever.create_retrieval_chainRetrieverThis chain takes in a user inquiry, which is then passed to the retriever to fetch relevant documents. Those documents (and original inputs) are then passed to an LLM to generate a responseLegacy Chains\\u200bBelow we report on the legacy chain types that exist. We will maintain\\nsupport for these until we are able to create a LCEL alternative. We\\nreport on:ChainName of the chain, or name of the constructor method. If constructor', metadata={'description': 'Chains refer to sequences of calls - whether to an LLM, a tool, or a', 'language': 'en', 'source': 'https://python.langchain.com/docs/modules/chains', 'title': 'Chains | ðŸ¦œï¸ðŸ”— Langchain'}),\n",
       "  Document(page_content='Skip to main contentDocsUse casesIntegrationsGuidesAPIMorePeopleVersioningChangelogContributingTemplatesCookbooksTutorialsYouTubeðŸ¦œï¸ðŸ”—LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsðŸ’¬SearchGet startedIntroductionInstallationQuickstartSecurityLangChain Expression LanguageGet startedWhy use LCELInterfaceStreamingHow toCookbookLangChain Expression Language (LCEL)ModulesModel I/ORetrievalAgentsChainsMoreLangServeLangSmithLangGraphModulesChainsChainsChains refer to sequences of calls - whether to an LLM, a tool, or a\\ndata preprocessing step. The primary supported way to do this is with\\nLCEL.LCEL is great for constructing your own chains, but itâ€™s also nice to\\nhave chains that you can use off-the-shelf. There are two types of\\noff-the-shelf chains that LangChain supports:Chains that are built with LCEL. In this case, LangChain offers a\\nhigher-level constructor method. However, all that is being done\\nunder the hood is constructing a chain with LCEL.[Legacy] Chains constructed by subclassing from a legacy Chain\\nclass. These chains do not use LCEL under the hood but are rather\\nstandalone classes.We are working creating methods that create LCEL versions of all chains.\\nWe are doing this for a few reasons.Chains constructed in this way are nice because if you want to\\nmodify the internals of a chain you can simply modify the LCEL.These chains natively support streaming, async, and batch out of the\\nbox.These chains automatically get observability at each step.This page contains two lists. First, a list of all LCEL chain\\nconstructors. Second, a list of all legacy Chains.LCEL Chains\\u200bBelow is a table of all LCEL chain constructors. In addition, we report\\non:Chain ConstructorThe constructor function for this chain. These are all methods that\\nreturn LCEL runnables. We also link to the API documentation.Function CallingWhether this requires OpenAI function calling.Other ToolsWhat other tools (if any) are used in this chain.When to UseOur commentary on when to use this chain.Chain ConstructorFunction CallingOther ToolsWhen to Usecreate_stuff_documents_chainThis chain takes a list of documents and formats them all into a prompt, then passes that prompt to an LLM. It passes ALL documents, so you should make sure it fits within the context window the LLM you are using.create_openai_fn_runnableâœ…If you want to use OpenAI function calling to OPTIONALLY structured an output response. You may pass in multiple functions for it call, but it does not have to call it.create_structured_output_runnableâœ…If you want to use OpenAI function calling to FORCE the LLM to respond with a certain function. You may only pass in one function, and the chain will ALWAYS return this response.load_query_constructor_runnableCan be used to generate queries. You must specify a list of allowed operations, and then will return a runnable that converts a natural language query into those allowed operations.create_sql_query_chainSQL DatabaseIf you want to construct a query for a SQL database from natural language.create_history_aware_retrieverRetrieverThis chain takes in conversation history and then uses that to generate a search query which is passed to the underlying retriever.create_retrieval_chainRetrieverThis chain takes in a user inquiry, which is then passed to the retriever to fetch relevant documents. Those documents (and original inputs) are then passed to an LLM to generate a responseLegacy Chains\\u200bBelow we report on the legacy chain types that exist. We will maintain\\nsupport for these until we are able to create a LCEL alternative. We\\nreport on:ChainName of the chain, or name of the constructor method. If constructor', metadata={'description': 'Chains refer to sequences of calls - whether to an LLM, a tool, or a', 'language': 'en', 'source': 'https://python.langchain.com/docs/modules/chains', 'title': 'Chains | ðŸ¦œï¸ðŸ”— Langchain'}),\n",
       "  Document(page_content='Skip to main contentDocsUse casesIntegrationsGuidesAPIMorePeopleVersioningChangelogContributingTemplatesCookbooksTutorialsYouTubeðŸ¦œï¸ðŸ”—LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsðŸ’¬SearchGet startedIntroductionInstallationQuickstartSecurityLangChain Expression LanguageGet startedWhy use LCELInterfaceStreamingHow toCookbookLangChain Expression Language (LCEL)ModulesModel I/ORetrievalAgentsChainsMoreLangServeLangSmithLangGraphModulesChainsChainsChains refer to sequences of calls - whether to an LLM, a tool, or a\\ndata preprocessing step. The primary supported way to do this is with\\nLCEL.LCEL is great for constructing your own chains, but itâ€™s also nice to\\nhave chains that you can use off-the-shelf. There are two types of\\noff-the-shelf chains that LangChain supports:Chains that are built with LCEL. In this case, LangChain offers a\\nhigher-level constructor method. However, all that is being done\\nunder the hood is constructing a chain with LCEL.[Legacy] Chains constructed by subclassing from a legacy Chain\\nclass. These chains do not use LCEL under the hood but are rather\\nstandalone classes.We are working creating methods that create LCEL versions of all chains.\\nWe are doing this for a few reasons.Chains constructed in this way are nice because if you want to\\nmodify the internals of a chain you can simply modify the LCEL.These chains natively support streaming, async, and batch out of the\\nbox.These chains automatically get observability at each step.This page contains two lists. First, a list of all LCEL chain\\nconstructors. Second, a list of all legacy Chains.LCEL Chains\\u200bBelow is a table of all LCEL chain constructors. In addition, we report\\non:Chain ConstructorThe constructor function for this chain. These are all methods that\\nreturn LCEL runnables. We also link to the API documentation.Function CallingWhether this requires OpenAI function calling.Other ToolsWhat other tools (if any) are used in this chain.When to UseOur commentary on when to use this chain.Chain ConstructorFunction CallingOther ToolsWhen to Usecreate_stuff_documents_chainThis chain takes a list of documents and formats them all into a prompt, then passes that prompt to an LLM. It passes ALL documents, so you should make sure it fits within the context window the LLM you are using.create_openai_fn_runnableâœ…If you want to use OpenAI function calling to OPTIONALLY structured an output response. You may pass in multiple functions for it call, but it does not have to call it.create_structured_output_runnableâœ…If you want to use OpenAI function calling to FORCE the LLM to respond with a certain function. You may only pass in one function, and the chain will ALWAYS return this response.load_query_constructor_runnableCan be used to generate queries. You must specify a list of allowed operations, and then will return a runnable that converts a natural language query into those allowed operations.create_sql_query_chainSQL DatabaseIf you want to construct a query for a SQL database from natural language.create_history_aware_retrieverRetrieverThis chain takes in conversation history and then uses that to generate a search query which is passed to the underlying retriever.create_retrieval_chainRetrieverThis chain takes in a user inquiry, which is then passed to the retriever to fetch relevant documents. Those documents (and original inputs) are then passed to an LLM to generate a responseLegacy Chains\\u200bBelow we report on the legacy chain types that exist. We will maintain\\nsupport for these until we are able to create a LCEL alternative. We\\nreport on:ChainName of the chain, or name of the constructor method. If constructor', metadata={'description': 'Chains refer to sequences of calls - whether to an LLM, a tool, or a', 'language': 'en', 'source': 'https://python.langchain.com/docs/modules/chains', 'title': 'Chains | ðŸ¦œï¸ðŸ”— Langchain'}),\n",
       "  Document(page_content='method, this will return a Chain subclass.Function CallingWhether this requires OpenAI Function Calling.Other ToolsOther tools used in the chain.When to UseOur commentary on when to use.ChainFunction CallingOther ToolsWhen to UseAPIChainRequests WrapperThis chain uses an LLM to convert a query into an API request, then executes that request, gets back a response, and then passes that request to an LLM to respondOpenAPIEndpointChainOpenAPI SpecSimilar to APIChain, this chain is designed to interact with APIs. The main difference is this is optimized for ease of use with OpenAPI endpointsConversationalRetrievalChainRetrieverThis chain can be used to have conversations with a document. It takes in a question and (optional) previous conversation history. If there is previous conversation history, it uses an LLM to rewrite the conversation into a query to send to a retriever (otherwise it just uses the newest user input). It then fetches those documents and passes them (along with the conversation) to an LLM to respond.StuffDocumentsChainThis chain takes a list of documents and formats them all into a prompt, then passes that prompt to an LLM. It passes ALL documents, so you should make sure it fits within the context window the LLM you are using.ReduceDocumentsChainThis chain combines documents by iterative reducing them. It groups documents into chunks (less than some context length) then passes them into an LLM. It then takes the responses and continues to do this until it can fit everything into one final LLM call. Useful when you have a lot of documents, you want to have the LLM run over all of them, and you can do in parallel.MapReduceDocumentsChainThis chain first passes each document through an LLM, then reduces them using the ReduceDocumentsChain. Useful in the same situations as ReduceDocumentsChain, but does an initial LLM call before trying to reduce the documents.RefineDocumentsChainThis chain collapses documents by generating an initial answer based on the first document and then looping over the remaining documents to refine its answer. This operates sequentially, so it cannot be parallelized. It is useful in similar situatations as MapReduceDocuments Chain, but for cases where you want to build up an answer by refining the previous answer (rather than parallelizing calls).MapRerankDocumentsChainThis calls on LLM on each document, asking it to not only answer but also produce a score of how confident it is. The answer with the highest confidence is then returned. This is useful when you have a lot of documents, but only want to answer based on a single document, rather than trying to combine answers (like Refine and Reduce methods do).ConstitutionalChainThis chain answers, then attempts to refine its answer based on constitutional principles that are provided. Use this when you want to enforce that a chainâ€™s answer follows some principles.LLMChainElasticsearchDatabaseChainElasticSearch InstanceThis chain converts a natural language question to an ElasticSearch query, and then runs it, and then summarizes the response. This is useful for when you want to ask natural language questions of an Elastic Search databaseFlareChainThis implements FLARE, an advanced retrieval technique. It is primarily meant as an exploratory advanced retrieval method.ArangoGraphQAChainArango GraphThis chain constructs an Arango query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.GraphCypherQAChainA graph that works with Cypher query languageThis chain constructs an Cypher query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.FalkorDBGraphQAChainFalkor DatabaseThis chain constructs a FalkorDB query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.HugeGraphQAChainHugeGraphThis chain constructs an HugeGraph query from natural language,', metadata={'description': 'Chains refer to sequences of calls - whether to an LLM, a tool, or a', 'language': 'en', 'source': 'https://python.langchain.com/docs/modules/chains', 'title': 'Chains | ðŸ¦œï¸ðŸ”— Langchain'})],\n",
       " 'answer': 'LCEL is considered a great tool for constructing chains for several reasons:\\n1. It allows you to easily construct your own chains.\\n2. It natively supports streaming, async, and batch processing.\\n3. Chains created with LCEL automatically have observability at each step.\\n4. It provides a higher-level constructor method for building off-the-shelf chains.\\n5. It offers flexibility for modifying the internals of a chain by simply modifying the LCEL.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # First we need a prompt that we can pass into an LLM to generate this search query\n",
    "\n",
    "# # The llm\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# # Build the vector database from document and set as retriever\n",
    "# vector_db = util.build_db()\n",
    "# retriever = vector_db.as_retriever()\n",
    "\n",
    "# # Engineer the prompt so it hold a conversation history\n",
    "# query_gen_prompt = ChatPromptTemplate.from_messages([\n",
    "#     MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "#     (\"user\", \"{input}\"),\n",
    "#     (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\")\n",
    "# ])\n",
    "\n",
    "# # This retrieves chain link the llm, the database retriever and the prompt.\n",
    "# # This retrieves chain will generate a search query based on the conversation history.\n",
    "# query_gen_retriever_chain = create_history_aware_retriever(llm, retriever, query_gen_prompt)\n",
    "\n",
    "# # Uncomment the following lines to test the chain.\n",
    "# # chat_history = [HumanMessage(content=\"Is LCEL a great tool?\"), AIMessage(content=\"Yes!\")]\n",
    "\n",
    "# # # Now we passing the chat history to the chain. \n",
    "# # # The chain will generate a search query based on the conversation history\n",
    "# # # Hence. It is not the final answer to the question, but a search query that can be used to get the answer.\n",
    "# # reponse = query_gen_retriever_chain.invoke({\n",
    "# #     \"chat_history\": chat_history,\n",
    "# #     \"input\": \"Tell me how\"\n",
    "# # })\n",
    "\n",
    "\n",
    "# # Now we combine the retriever chain that generate query with the document chain to generate the final answer.\n",
    "# # The document chain will generate the final answer based on the search query generated by the retriever chain.\n",
    "# final_prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
    "#     MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "#     (\"user\", \"{input}\"),\n",
    "# ])\n",
    "# document_chain = create_stuff_documents_chain(llm, final_prompt)\n",
    "\n",
    "# final_retrieval_chain = create_retrieval_chain(query_gen_retriever_chain, document_chain)\n",
    "\n",
    "# # Uncomment the following lines to test the chain.\n",
    "# chat_history = [HumanMessage(content=\"Is LCEL a great tool?\"), AIMessage(content=\"Yes!\")]\n",
    "# final_retrieval_chain.invoke({\n",
    "#     \"chat_history\": chat_history,\n",
    "#     \"input\": \"Tell me how\"\n",
    "# })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent\n",
    "The agent has abaility to search on web or reterive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `tavily_search_results_json` with `{'query': 'weather in San Francisco'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m[{'url': 'https://forecast.weather.gov/zipcity.php?inputstring=San francisco,CA', 'content': 'NOAA National Weather Service National Weather Service. Toggle navigation. HOME; FORECAST . Local; Graphical; Aviation; Marine; Rivers and Lakes; Hurricanes; Severe Weather; Fire Weather; ... San Francisco CA 37.77Â°N 122.41Â°W (Elev. 131 ft) Last Update: 2:21 am PDT Mar 27, 2024. Forecast Valid: 7am PDT Mar 27, 2024-6pm PDT Apr 2, 2024 .'}, {'url': 'https://www.wunderground.com/forecast/us/ca/san-francisco', 'content': 'Get the latest 10-day weather forecast for San Francisco, CA, including temperature, precipitation, wind, pressure, humidity and more. See current conditions, sunrise and sunset times, moon phases and nearby weather stations.'}, {'url': 'https://www.accuweather.com/en/us/san-francisco/94103/weather-forecast/347629', 'content': 'Get the current and future weather conditions for San Francisco, CA, including temperature, precipitation, wind, air quality and more. See the hourly and 10-day outlook, radar maps, alerts and allergy information.'}, {'url': 'https://weather.com/weather/tenday/l/San Francisco CA USCA0987:1:US', 'content': \"Comfy & Cozy\\nThat's Not What Was Expected\\nOutside\\n'No-Name Storms' In Florida\\nGifts From On High\\nWhat To Do For Wheezing\\nSurviving The Season\\nStay Safe\\nAir Quality Index\\nAir quality is considered satisfactory, and air pollution poses little or no risk.\\n Health & Activities\\nSeasonal Allergies and Pollen Count Forecast\\nNo pollen detected in your area\\nCold & Flu Forecast\\nFlu risk is low in your area\\nWe recognize our responsibility to use data and technology for good. recents\\nSpecialty Forecasts\\n10 Day Weather-San Francisco, CA\\nToday\\nMon 18 | Day\\nConsiderable cloudiness. Tue 19\\nTue 19 | Day\\nLight rain early...then remaining cloudy with showers in the afternoon. Wed 27\\nWed 27 | Day\\nOvercast with rain showers at times.\"}, {'url': 'https://weather.com/weather/hourbyhour/l/USCA0987:1:US', 'content': \"recents\\nSpecialty Forecasts\\nHourly Weather-San Francisco, CA\\nBeach Hazard Statement\\nSaturday, November 25\\n5 pm\\nClear\\n6 pm\\nClear\\n7 pm\\nClear\\n8 pm\\nMostly Clear\\n9 pm\\nPartly Cloudy\\n10 pm\\nPartly Cloudy\\n11 pm\\nPartly Cloudy\\nSunday, November 26\\n12 am\\nPartly Cloudy\\n1 am\\nMostly Cloudy\\n2 am\\nMostly Cloudy\\n3 am\\nMostly Cloudy\\n4 am\\nCloudy\\n5 am\\nCloudy\\n6 am\\nMostly Cloudy\\n7 am\\nMostly Cloudy\\n8 am\\nMostly Cloudy\\n9 am\\nMostly Cloudy\\n10 am\\nPartly Cloudy\\n11 am\\nPartly Cloudy\\n12 pm\\nPartly Cloudy\\n1 pm\\nPartly Cloudy\\n2 pm\\nPartly Cloudy\\n3 pm\\nPartly Cloudy\\n4 pm\\nPartly Cloudy\\n5 pm\\nPartly Cloudy\\n6 pm\\nPartly Cloudy\\n7 pm\\nPartly Cloudy\\n8 pm\\nPartly Cloudy\\n9 pm\\nMostly Clear\\n10 pm\\nMostly Clear\\n11 pm\\nMostly Clear\\nMonday, November 27\\n12 am\\nClear\\n1 am\\nMostly Clear\\n2 am\\nMostly Clear\\n3 am\\nPartly Cloudy\\n4 am\\nMostly Clear\\n5 am\\nMostly Clear\\n6 am\\nClear\\n7 am\\nClear\\n8 am\\nSunny\\n9 am\\nSunny\\n10 am\\nSunny\\n11 am\\nSunny\\n12 pm\\nMostly Sunny\\n1 pm\\nPartly Cloudy\\n2 pm\\nMostly Sunny\\n3 pm\\nMostly Sunny\\n4 pm\\nPartly Cloudy\\nRadar\\nSafety First!\\n Don't Miss\\nIrresistible\\nWeather Wonders\\nOur Amazing World\\nCelestial Symphony\\nFried Turkey Fail\\nLook At That!\\n Health & Activities\\nSeasonal Allergies and Pollen Count Forecast\\nNo pollen detected in your area\\nCold & Flu Forecast\\nFlu risk is low in your area\\nWe recognize our responsibility to use data and technology for good. Changes For Critters\\nHurricane Tracker\\nStay Safe\\nAir Quality Index\\nAir quality is considered satisfactory, and air pollution poses little or no risk.\\n Take control of your data.\\n\"}]\u001b[0m\u001b[32;1m\u001b[1;3mI found some sources where you can check the weather in San Francisco:\n",
      "\n",
      "1. [NOAA National Weather Service](https://forecast.weather.gov/zipcity.php?inputstring=San%20francisco,CA): Provides local forecasts for San Francisco.\n",
      "2. [Weather Underground](https://www.wunderground.com/forecast/us/ca/san-francisco): Offers a 10-day weather forecast for San Francisco, including temperature, precipitation, wind, and more.\n",
      "3. [AccuWeather](https://www.accuweather.com/en/us/san-francisco/94103/weather-forecast/347629): Provides current and future weather conditions for San Francisco, including temperature, precipitation, wind, and air quality.\n",
      "4. [The Weather Channel](https://weather.com/weather/tenday/l/San%20Francisco%20CA%20USCA0987:1:US): Offers a 10-day weather forecast for San Francisco with hourly updates.\n",
      "5. [Hourly Weather Forecast](https://weather.com/weather/hourbyhour/l/USCA0987:1:US): Provides an hourly breakdown of the weather in San Francisco.\n",
      "\n",
      "You can visit these websites to get detailed information about the weather in San Francisco.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is the weather in SF?',\n",
       " 'output': 'I found some sources where you can check the weather in San Francisco:\\n\\n1. [NOAA National Weather Service](https://forecast.weather.gov/zipcity.php?inputstring=San%20francisco,CA): Provides local forecasts for San Francisco.\\n2. [Weather Underground](https://www.wunderground.com/forecast/us/ca/san-francisco): Offers a 10-day weather forecast for San Francisco, including temperature, precipitation, wind, and more.\\n3. [AccuWeather](https://www.accuweather.com/en/us/san-francisco/94103/weather-forecast/347629): Provides current and future weather conditions for San Francisco, including temperature, precipitation, wind, and air quality.\\n4. [The Weather Channel](https://weather.com/weather/tenday/l/San%20Francisco%20CA%20USCA0987:1:US): Offers a 10-day weather forecast for San Francisco with hourly updates.\\n5. [Hourly Weather Forecast](https://weather.com/weather/hourbyhour/l/USCA0987:1:US): Provides an hourly breakdown of the weather in San Francisco.\\n\\nYou can visit these websites to get detailed information about the weather in San Francisco.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The llm\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Build the vector database from document and set as retriever\n",
    "vector_db = util.build_db()\n",
    "retriever = vector_db.as_retriever()\n",
    "\n",
    "# The tool that will be used to search for information about our document, passing a name and description. \n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"chain_search\",\n",
    "    \"Search for information about Chains. For any questions about Chain, you must use this tool!\",\n",
    ")\n",
    "\n",
    "# The expensive search tool....\n",
    "search = TavilySearchResults()\n",
    "\n",
    "tools = [retriever_tool, search]\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "agent = create_openai_functions_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "# First test, ask a question that need apply a seach online\n",
    "agent_executor.invoke({\"input\": \"what is the weather in SF?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `chain_search` with `{'query': 'LCEL chain'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mSkip to main contentDocsUse casesIntegrationsGuidesAPIMorePeopleVersioningChangelogContributingTemplatesCookbooksTutorialsYouTubeðŸ¦œï¸ðŸ”—LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsðŸ’¬SearchGet startedIntroductionInstallationQuickstartSecurityLangChain Expression LanguageGet startedWhy use LCELInterfaceStreamingHow toCookbookLangChain Expression Language (LCEL)ModulesModel I/ORetrievalAgentsChainsMoreLangServeLangSmithLangGraphModulesChainsChainsChains refer to sequences of calls - whether to an LLM, a tool, or a\n",
      "data preprocessing step. The primary supported way to do this is with\n",
      "LCEL.LCEL is great for constructing your own chains, but itâ€™s also nice to\n",
      "have chains that you can use off-the-shelf. There are two types of\n",
      "off-the-shelf chains that LangChain supports:Chains that are built with LCEL. In this case, LangChain offers a\n",
      "higher-level constructor method. However, all that is being done\n",
      "under the hood is constructing a chain with LCEL.[Legacy] Chains constructed by subclassing from a legacy Chain\n",
      "class. These chains do not use LCEL under the hood but are rather\n",
      "standalone classes.We are working creating methods that create LCEL versions of all chains.\n",
      "We are doing this for a few reasons.Chains constructed in this way are nice because if you want to\n",
      "modify the internals of a chain you can simply modify the LCEL.These chains natively support streaming, async, and batch out of the\n",
      "box.These chains automatically get observability at each step.This page contains two lists. First, a list of all LCEL chain\n",
      "constructors. Second, a list of all legacy Chains.LCEL Chainsâ€‹Below is a table of all LCEL chain constructors. In addition, we report\n",
      "on:Chain ConstructorThe constructor function for this chain. These are all methods that\n",
      "return LCEL runnables. We also link to the API documentation.Function CallingWhether this requires OpenAI function calling.Other ToolsWhat other tools (if any) are used in this chain.When to UseOur commentary on when to use this chain.Chain ConstructorFunction CallingOther ToolsWhen to Usecreate_stuff_documents_chainThis chain takes a list of documents and formats them all into a prompt, then passes that prompt to an LLM. It passes ALL documents, so you should make sure it fits within the context window the LLM you are using.create_openai_fn_runnableâœ…If you want to use OpenAI function calling to OPTIONALLY structured an output response. You may pass in multiple functions for it call, but it does not have to call it.create_structured_output_runnableâœ…If you want to use OpenAI function calling to FORCE the LLM to respond with a certain function. You may only pass in one function, and the chain will ALWAYS return this response.load_query_constructor_runnableCan be used to generate queries. You must specify a list of allowed operations, and then will return a runnable that converts a natural language query into those allowed operations.create_sql_query_chainSQL DatabaseIf you want to construct a query for a SQL database from natural language.create_history_aware_retrieverRetrieverThis chain takes in conversation history and then uses that to generate a search query which is passed to the underlying retriever.create_retrieval_chainRetrieverThis chain takes in a user inquiry, which is then passed to the retriever to fetch relevant documents. Those documents (and original inputs) are then passed to an LLM to generate a responseLegacy Chainsâ€‹Below we report on the legacy chain types that exist. We will maintain\n",
      "support for these until we are able to create a LCEL alternative. We\n",
      "report on:ChainName of the chain, or name of the constructor method. If constructor\n",
      "\n",
      "Skip to main contentDocsUse casesIntegrationsGuidesAPIMorePeopleVersioningChangelogContributingTemplatesCookbooksTutorialsYouTubeðŸ¦œï¸ðŸ”—LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsðŸ’¬SearchGet startedIntroductionInstallationQuickstartSecurityLangChain Expression LanguageGet startedWhy use LCELInterfaceStreamingHow toCookbookLangChain Expression Language (LCEL)ModulesModel I/ORetrievalAgentsChainsMoreLangServeLangSmithLangGraphModulesChainsChainsChains refer to sequences of calls - whether to an LLM, a tool, or a\n",
      "data preprocessing step. The primary supported way to do this is with\n",
      "LCEL.LCEL is great for constructing your own chains, but itâ€™s also nice to\n",
      "have chains that you can use off-the-shelf. There are two types of\n",
      "off-the-shelf chains that LangChain supports:Chains that are built with LCEL. In this case, LangChain offers a\n",
      "higher-level constructor method. However, all that is being done\n",
      "under the hood is constructing a chain with LCEL.[Legacy] Chains constructed by subclassing from a legacy Chain\n",
      "class. These chains do not use LCEL under the hood but are rather\n",
      "standalone classes.We are working creating methods that create LCEL versions of all chains.\n",
      "We are doing this for a few reasons.Chains constructed in this way are nice because if you want to\n",
      "modify the internals of a chain you can simply modify the LCEL.These chains natively support streaming, async, and batch out of the\n",
      "box.These chains automatically get observability at each step.This page contains two lists. First, a list of all LCEL chain\n",
      "constructors. Second, a list of all legacy Chains.LCEL Chainsâ€‹Below is a table of all LCEL chain constructors. In addition, we report\n",
      "on:Chain ConstructorThe constructor function for this chain. These are all methods that\n",
      "return LCEL runnables. We also link to the API documentation.Function CallingWhether this requires OpenAI function calling.Other ToolsWhat other tools (if any) are used in this chain.When to UseOur commentary on when to use this chain.Chain ConstructorFunction CallingOther ToolsWhen to Usecreate_stuff_documents_chainThis chain takes a list of documents and formats them all into a prompt, then passes that prompt to an LLM. It passes ALL documents, so you should make sure it fits within the context window the LLM you are using.create_openai_fn_runnableâœ…If you want to use OpenAI function calling to OPTIONALLY structured an output response. You may pass in multiple functions for it call, but it does not have to call it.create_structured_output_runnableâœ…If you want to use OpenAI function calling to FORCE the LLM to respond with a certain function. You may only pass in one function, and the chain will ALWAYS return this response.load_query_constructor_runnableCan be used to generate queries. You must specify a list of allowed operations, and then will return a runnable that converts a natural language query into those allowed operations.create_sql_query_chainSQL DatabaseIf you want to construct a query for a SQL database from natural language.create_history_aware_retrieverRetrieverThis chain takes in conversation history and then uses that to generate a search query which is passed to the underlying retriever.create_retrieval_chainRetrieverThis chain takes in a user inquiry, which is then passed to the retriever to fetch relevant documents. Those documents (and original inputs) are then passed to an LLM to generate a responseLegacy Chainsâ€‹Below we report on the legacy chain types that exist. We will maintain\n",
      "support for these until we are able to create a LCEL alternative. We\n",
      "report on:ChainName of the chain, or name of the constructor method. If constructor\n",
      "\n",
      "Chains | ðŸ¦œï¸ðŸ”— Langchain\n",
      "\n",
      "Chains | ðŸ¦œï¸ðŸ”— Langchain\u001b[0m\u001b[32;1m\u001b[1;3mThe LCEL chain is a great tool for constructing your own chains and also for using off-the-shelf chains. It allows you to easily modify the internals of a chain, supports streaming, async, and batch operations out of the box, and provides observability at each step. It offers various chain constructors such as creating documents chains, SQL query chains, retriever chains, and more. Overall, the LCEL chain is a versatile and powerful tool for building and executing chains of operations.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='Is LCEL chain a great tool?'),\n",
       "  AIMessage(content='Yes!')],\n",
       " 'input': 'Tell me how',\n",
       " 'output': 'The LCEL chain is a great tool for constructing your own chains and also for using off-the-shelf chains. It allows you to easily modify the internals of a chain, supports streaming, async, and batch operations out of the box, and provides observability at each step. It offers various chain constructors such as creating documents chains, SQL query chains, retriever chains, and more. Overall, the LCEL chain is a versatile and powerful tool for building and executing chains of operations.'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second task. Ask a question that related to the document.\n",
    "chat_history = [HumanMessage(content=\"Is LCEL chain a great tool?\"), AIMessage(content=\"Yes!\")]\n",
    "agent_executor.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})\n",
    "\n",
    "# As the result, the agent uses different resources to get the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
